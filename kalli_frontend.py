import os
import gradio as gr
from supabase import create_client
from dotenv import load_dotenv
from openai import OpenAI

# ğŸŒ± Umgebungsvariablen laden
load_dotenv()

SUPABASE_URL = os.environ["SUPABASE_URL"]
SUPABASE_KEY = os.environ["SUPABASE_SERVICE_ROLE"]
OPENAI_API_KEY = os.environ["OPENAI_API_KEY"]

# ğŸ”Œ Clients initialisieren
supabase = create_client(SUPABASE_URL, SUPABASE_KEY)
openai_client = OpenAI(api_key=OPENAI_API_KEY)

# ğŸ” Semantische Abfrage an match_bvv_dokumente
def frage_kalli(prompt):
    print(f"\nğŸŸ¡ Neue Anfrage: {prompt}")

    try:
        # ğŸ§  Embedding erzeugen (neue Syntax!)
        response = openai_client.embeddings.create(
            input=prompt,
            model="text-embedding-3-small"
        )
        embedding = response.data[0].embedding
        print(f"ğŸ”¹ Embedding erzeugt. LÃ¤nge: {len(embedding)}")

        # ğŸ“¡ RPC ausfÃ¼hren
        response = supabase.rpc(
            "match_bvv_dokumente",
            {
                "query_embedding": embedding,
                "match_threshold": 0.4,
                "match_count": 3
            }
        ).execute()

        matches = response.data or []
        print(f"ğŸŸ¢ Treffer: {len(matches)}")

        if not matches:
            return "ğŸ˜• Leider keine passenden Inhalte gefunden."

        # âœï¸ Ausgabe formatieren
        antwort = "\n\n".join([
            f"ğŸ“Œ **{m['titel']}** ({m['kategorie']}, {m['datum']})\n{m.get('inhalt', '').strip()}"
            for m in matches
        ])
        return antwort

    except Exception as e:
        print(f"ğŸ”´ Fehler bei Anfrage: {e}")
        return f"ğŸ’¥ Fehler bei der Verarbeitung: {e}"

# ğŸ›ï¸ Gradio UI starten
iface = gr.Interface(
    fn=frage_kalli,
    inputs=gr.Textbox(label="Was willst Du wissen?", placeholder="Stell mir eine Frageâ€¦"),
    outputs=gr.Markdown(label="Antwort von KalliGPT"),
    title="KalliGPT ğŸ§ ",
    description="Frage Kalli nach BVV-AntrÃ¤gen und Anfragen â€“ direkt aus der Datenbank!"
)

iface.launch()
